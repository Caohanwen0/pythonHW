[TOC]

# 数据分析

## 上传时间

利用python的`datetime`分析视频的上传日期，并使用`matplotlib`绘制饼图显示。

```python
def analyse_date(): #分析作者们都是星期几发视频
    label = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']
    count = np.zeros(7, dtype='int')
    for video in Video.objects.all():
        date_obj = datetime.datetime.strptime(video.datePublished, "%Y-%m-%d")
        count[int(datetime.datetime.strftime(date_obj, '%w'))] += 1
    plt.pie(count, labels = label, autopct="%.2f%%",explode=[0.1,0,0,0,0,0,0.1])
    plt.axis('equal')
    plt.title('Video Published Time')
    plt.legend()
    plt.savefig('analyse_date.jpg')
    plt.show()
```

![](https://pic.imgdb.cn/item/6134ce1e44eaada739fb2198.jpg)

从图中可以看出，相比周末，视频作者更倾向于在周三和周四上传视频。周末上传的视频仅仅占21.99%(不足平均值$\frac{2}{7}=$28.57%)。

## 粉丝数与更新间隔

分析了作者的更新频率（相邻发布的两个视频的平均间隔时间）和粉丝数量的关系，并绘制散点图。

为了将作者的所有视频以发布时间排列，使用了`datetime`的函数`.toordinal()`,以及`np`的排序方法。

```python
def analyse_intern(): #分析作者更新频率和粉丝数
    intern_list = []
    follower_list = []
    last_date_obj = datetime.date(1,1,1)
    for author_ in Author.objects.all():
        time = [] #保存所有更新intern的一个list
        video_list = Video.objects.filter(author = author_)
        for video in video_list:
            temp = temp = datetime.datetime.strptime(video.datePublished, "%Y-%m-%d").toordinal()
            time.append(temp)
        time_np = np.array(time)
        time_np.sort()
        intern = []
        for i in range(1,len(time_np)):
            temp = time_np[i]-time_np[i-1]
            intern.append(temp)
        intern_np = np.array(intern)
        if intern_np.mean()>800: 
            continue
        else:
            intern_list.append(intern_np.mean())
        #下面处理author_.followers
        if author_.follower[-2]!='K':
            if author_.follower[2:-1]=='':
                follower = 0
            else: 
                follower = int(author_.follower[2:-1])
        else:
            follower = int(author_.follower.replace('.','').replace('K','')[2:-1])*100
        follower_list.append(follower) 
    plt.xlabel('Update intern/Day')
    plt.ylabel('Follower popularity/Man')
    plt.title('The correlation between update intern and followers')
    colors = np.random.rand(685)
    plt.scatter(intern_list,follower_list,marker="x",s=20, c=colors,alpha=0.5,) #marker是点的样式，s是点的大小,c是颜色（默认是蓝色）。
    plt.savefig('analyse_favor.jpg')
    plt.show()
```

![](https://pic.imgdb.cn/item/6134ce1e44eaada739fb217d.jpg)

可以看出，随着更新频率增加，粉丝数量也相对增加。但是，仍然存在大量更新频繁但粉丝数并不多的情况。

## 词频分析

通过配合使用库`wordcloud`及`plt`,生成了所有扒取的评论的词频云图。

云图生成代码如下。首先将所有的评论集合成一个字符串，然后将字符串的换行符和其他特殊符号清除，得到净文本。以该文本生成一个`WordCloud`类对象，并在构造函数中设置图片大小为3000x2000、背景颜色为黑色、关键字颜色、不允许关键字重复、去除无意义词汇（使用`STOPWORDS`)。最后通过`matplotlib.pyplot`加载并保存该实例。

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud,STOPWORDS

def analyse_word():
    # get text
    text = ''
    for video in Video.objects.all():
        if video.comment_one!='':
            text += video.comment_one
        if video.comment_two!='':
            text += video.comment_two
        if video.comment_one!='':
            text += video.comment_three
        if video.comment_four!='':
            text += video.comment_four
        if video.comment_five!='':
            text += video.comment_five
    # clean text
    text = re.sub(r'==.*?==+', '',text)
    text = text.replace('\n', '')
    # generate cloud
    wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', \
        colormap='Set2',collocations=False, stopwords=STOPWORDS).generate(text)
    plt.figure(figsize=(40,30))
    plt.imshow(wordcloud)
    plt.axis('off')
    plt.savefig('analyse_word.jpg') 
    plt.show()
```

<img src="https://pic.imgdb.cn/item/6134b3be44eaada739c436fb.jpg" alt="x" style="zoom:80%;" />

如果不借用`wordCloud`库，我们也可以使用正则表达式和`numpy`分析关键词出现频率。首先用`re.split`方法将文本断开为词，然后使用`pd.Series.value_counts()`方法生成一个`Series`对象，其index为词条，value则为各词出现的次数。

```python
		#用和前面一样的方式得到text
    all_words = re.split('\W', text)
    word_list = pd.Series(all_words).value_counts().head(100)
    STOPWORDS.update(['I','The','This','s','t','m'])
    for ind in word_list.index:
        if ind not in STOPWORDS:
            print(ind,word_list[ind])
```

仍然借用`STOPWORDS`库，我们去除无意义词，以及由于文本处理出现的‘s’,’t’,’m’（在出现It’s ,I’m , don’t 这种表达的时候split函数会把它们从’断开）后，得到的出现最频繁的100个词中剩余的词及其出现次数如下：

```bash
love 1438
season 1398
show 1273
one 1185
episode 839
think 837
really 795
It 782
see 722
good 713
will 684
que 673
much 634
best 609
de 603
video 576
time 552
know 538
scene 533
ve 531
back 504
AHS 503
great 497
o 475
don 466
first 438
character 435
people 434
```

这和云图所反映的情况大致符合。

可以看出，两种方法得到的评论词汇可以大致分为三类：

首先是反映观众情感的词，包括really,good,love,best,great,amazing,lol等。最常出现的情感词全部为正面评价。

其次是关于剧集情况的词，包括series,episode,season等。显然，在一部剧还没有完结的时候，观众关心续订情况和更新时间；在一部剧完结后，观众则会惋惜其拍得不够多。

最后是关于剧情的词。由于我选取的样本关键词为Person of Interest、American Horror Story和Good Omens,可以看到包括John,Harold,Root(POI角色名字),Coven(AHS第三季剧名)等字样。



